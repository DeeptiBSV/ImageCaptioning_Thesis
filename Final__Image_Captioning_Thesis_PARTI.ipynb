{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_ Image_Captioning-Thesis_PARTI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtv1NzACb9np"
      },
      "source": [
        "IMAGE CAPTIONING_DISSERTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D36ci5Ft3OpZ"
      },
      "source": [
        "#Import libraries\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from time import time\n",
        "import keras\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "u26NfTRkrtF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4V7kJW2IL6t"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc1oHoer4Q_M"
      },
      "source": [
        "DATA Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJnIAb333wo-"
      },
      "source": [
        "##download train dataset\n",
        "!gdown --id \"1GSxuTJpgtCoJUOTLLmlJyIri3JXkjYL9\"\n",
        "##download test dataset\n",
        "!gdown --id \"1Uj2jSbSggJfsLVJDDlxvHOUxb0BDuxij\"\n",
        "##download token dataset\n",
        "!gdown --id \"1o6S-v9W2YXnhhlp2CzdLrPvxFyJG8uxb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqFBLx2Q3w2-"
      },
      "source": [
        "trainpath=\"/content/Flickr_8k.trainImages.txt\"\n",
        "testpath=\"/content/Flickr_8k.testImages.txt\"\n",
        "tokenpath=\"/content/Flickr8k.token.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0eLZKV-3w5c"
      },
      "source": [
        "doc = open(tokenpath,'r').read()\n",
        "print(doc[:410])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmBtgkUdfe-w"
      },
      "source": [
        "Creating Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy8CNp8d3w8K"
      },
      "source": [
        "descriptions = dict()\n",
        "for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(line) > 2:\n",
        "          imageid = tokens[0].split('.')[0]\n",
        "          imagedesc = ' '.join(tokens[1:])\n",
        "          if imageid not in descriptions:\n",
        "              descriptions[imageid] = list()\n",
        "          descriptions[imageid].append(imagedesc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zbGV1W63w-y"
      },
      "source": [
        "##Text Cleaning\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "for key, desc_list in descriptions.items():\n",
        "    for i in range(len(desc_list)):\n",
        "        desc = desc_list[i]\n",
        "        desc = desc.split()\n",
        "        desc = [word.lower() for word in desc]\n",
        "        desc = [w.translate(table) for w in desc]\n",
        "        desc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em7JzrWhvReO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhH4NUQ52vai"
      },
      "source": [
        "imagepath='/content/drive/MyDrive/ImageDataset/Flicker8k_Dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRPy5D423xBg"
      },
      "source": [
        "\n",
        "##Visualizing the image and checking its captions\n",
        "pic = '1000268201_693b08cb0e.jpg'\n",
        "x=plt.imread(imagepath+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "descriptions['1000268201_693b08cb0e']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEGUBsnu3xD8"
      },
      "source": [
        "#Creating vocabulary for all Unique words present across\n",
        "vocabulary = set()\n",
        "for key in descriptions.keys():\n",
        "        [vocabulary.update(d.split()) for d in descriptions[key]]\n",
        "print('Original Vocabulary Size: %d' % len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IuCwKbn3xHX"
      },
      "source": [
        "#Saving the image id and its cleaned caption in the same format as it was having a token format\n",
        "lines = list()\n",
        "for key, desc_list in descriptions.items():\n",
        "    for desc in desc_list:\n",
        "        lines.append(key + ' ' + desc)\n",
        "new_descriptions = '\\n'.join(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Tax_kDZ3p1"
      },
      "source": [
        "new_descriptions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRGgVlNCV0qL"
      },
      "source": [
        "#Loading all the train image ids from flickr8k_trainimages in train dataset\n",
        "doc = open(trainpath,'r').read()\n",
        "dataset = list()\n",
        "for line in doc.split('\\n'):\n",
        "    if len(line) > 1:\n",
        "      identifier = line.split('.')[0]\n",
        "      dataset.append(identifier)\n",
        "\n",
        "train = set(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ZJ73NGWDuv"
      },
      "source": [
        "#saving the training and test images in the respective variables train_img and test_img\n",
        "img = glob.glob(imagepath + '*.jpg')\n",
        "train_images = set(open(trainpath, 'r').read().strip().split('\\n'))\n",
        "train_img = []\n",
        "for i in img: \n",
        "    if i[len(imagepath):] in train_images:\n",
        "        train_img.append(i)\n",
        "\n",
        "test_images = set(open(testpath, 'r').read().strip().split('\\n'))\n",
        "test_img = []\n",
        "for i in img: \n",
        "    if i[len(imagepath):] in test_images: \n",
        "        test_img.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading all the test image ids from flickr8k_trainimages in train dataset\n",
        "doc = open(trainpath,'r').read()\n",
        "dataset = list()\n",
        "for line in doc.split('\\n'):\n",
        "    if len(line) > 1:\n",
        "      identifier = line.split('.')[0]\n",
        "      dataset.append(identifier)\n",
        "\n",
        "test = set(dataset)"
      ],
      "metadata": {
        "id": "wf3qck5N8zNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MJmKbfVWD8C"
      },
      "source": [
        "#loading the descriptions of train images in to dictionary and add two tokens called startseq and endseq\n",
        "desc_train = dict()\n",
        "for line in new_descriptions.split('\\n'):\n",
        "    tokens = line.split()\n",
        "    imageid, imagedesc = tokens[0], tokens[1:]\n",
        "    if imageid in train:\n",
        "        if imageid not in desc_train:\n",
        "            desc_train[imageid] = list()\n",
        "        desc = 'startseq ' + ' '.join(imagedesc) + ' endseq'\n",
        "        desc_train[imageid].append(desc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq2KJVX-WEGE"
      },
      "source": [
        "#creating the list all the training captions\n",
        "all_train_captions = []\n",
        "for key, val in desc_train.items():\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the descriptions of test images in to dictionary and add two tokens called startseq and endseq\n",
        "test_descriptions = dict()\n",
        "for line in new_descriptions.split('\\n'):\n",
        "    tokens = line.split()\n",
        "    imageid, imagedesc = tokens[0], tokens[1:]\n",
        "    if imageid in test:\n",
        "        if imageid not in test_descriptions:\n",
        "            test_descriptions[imageid] = list()\n",
        "        desc = ' '.join(imagedesc) \n",
        "        test_descriptions[imageid].append(desc)\n",
        "#creating the list all the test captions\n",
        "all_test_captions = []\n",
        "for key, val in test_descriptions.items():\n",
        "    for cap in val:\n",
        "        all_test_captions.append(cap)"
      ],
      "metadata": {
        "id": "BX1hm_Wi8o27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPFH2TB_d5oj"
      },
      "source": [
        "##checking train and test captions\n",
        "all_train_captions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_test_captions"
      ],
      "metadata": {
        "id": "bFmNiicE-MSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8caKrvsWEJB"
      },
      "source": [
        "#reducing the vocabulary to only those words which occur atleast 8 times in the entire corpus which will make the model more robbust\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(vocab)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6DZg1lQWEMy"
      },
      "source": [
        "#creating two dictionary to map index to words and viceversa\n",
        "ix2word = {}\n",
        "word2ix = {}\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    word2ix[w] = ix\n",
        "    ix2word[ix] = w\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ix2word) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT7dtX4TWEPQ"
      },
      "source": [
        "##need to find the max length of a caption \n",
        "all_desc = list()\n",
        "for key in desc_train.keys():\n",
        "    [all_desc.append(d) for d in desc_train[key]]\n",
        "lines = all_desc\n",
        "max_length = max(len(d.split()) for d in lines)\n",
        "\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EesTzx7hfele"
      },
      "source": [
        "##200d glove embeddings\n",
        "##With pretrained glove embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny4qZjYWfeoi"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT3y_71VjxuJ"
      },
      "source": [
        "path_to_glove=\"glove.6B.200d.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4UgLFMpfer8"
      },
      "source": [
        "embeddings_index = {} \n",
        "with open(path_to_glove, 'r', encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZJvPQ0FfevD"
      },
      "source": [
        "##with matrix shape of (1660,200) consisting of the vocabulary created and 200d vector\n",
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word2ix.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewa6CXwCk1RU"
      },
      "source": [
        "Building model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ0tYIJqk4Z6"
      },
      "source": [
        "##using transfer learning fromincception v3\n",
        "model = InceptionV3(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CCHIs-HlGcB"
      },
      "source": [
        "new_model = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fadidfAlhb2"
      },
      "source": [
        "def preprocess(imagepath):\n",
        "    img = image.load_img(imagepath, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kzmT8nSlhee"
      },
      "source": [
        "\n",
        "def encode(image):\n",
        "    image = preprocess(image) \n",
        "    feat_vect = new_model.predict(image) \n",
        "    feat_vect = np.reshape(feat_vect, feat_vect.shape[1])\n",
        "    return feat_vect\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c18laKj2N4r6"
      },
      "source": [
        "##code takes longer time to run\n",
        "encoding_train = {}\n",
        "for img in train_img:\n",
        "  encoding_train[img[len(imagepath):]] = encode(img)\n",
        "train_features = encoding_train \n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu7ekEVeN4yA"
      },
      "source": [
        "encoding_test = {}\n",
        "for img in test_img:\n",
        "    encoding_test[img[len(imagepath):]] = encode(img)\n",
        "test_features=encoding_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIoVy_PimBvN"
      },
      "source": [
        "#defining Model\n",
        "inputs1 = Input(shape=(2048,))\n",
        "f1 = Dropout(0.5)(inputs1)\n",
        "f2 = Dense(256, activation='relu')(f1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "s1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "s2 = Dropout(0.5)(s1)\n",
        "s3 = LSTM(256)(s2)\n",
        "\n",
        "decoder1 = add([f2, s3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJr7_5sllhhN"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qa9kz6ZlhkJ"
      },
      "source": [
        "#Model training\n",
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Junp-QVWlhmf"
      },
      "source": [
        "#compiling using categorical cross entropy\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf82OVi7lhpE"
      },
      "source": [
        "#Using batches for training the data\n",
        "def data_generator(descriptions, photos, word2ix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key+'.jpg']\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [word2ix[word] for word in desc.split(' ') if word in word2ix]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "            if n==num_photos_per_batch:\n",
        "                yield ([array(X1), array(X2)], array(y))\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR8mM9-5lhs0"
      },
      "source": [
        "#Using 30epoch with batch size as 1000\n",
        "epochs = 30   \n",
        "batch_size = 10\n",
        "steps = len(desc_train)//batch_size\n",
        "\n",
        "generator = data_generator(desc_train, train_features, word2ix, max_length, batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint"
      ],
      "metadata": {
        "id": "OKrLa0QJ5stw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0,patience = 3, verbose = 1,restore_best_weights = True)"
      ],
      "metadata": {
        "id": "0X9AZT5u5gja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(generator, epochs=epochs, steps_per_epoch=steps,callbacks=[earlystop])"
      ],
      "metadata": {
        "id": "uwAxDLyq5Lk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary"
      ],
      "metadata": {
        "id": "IPzS4HRt4DLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Disseration_SharedDrive/model1.h5\")"
      ],
      "metadata": {
        "id": "IRw3jveH1PFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the saved model and using it in future\n",
        "#Model1download\n",
        "!gdown --id \"1-4fC11e9i3IfSj1IoAnrL8Xv0kUCafUm\""
      ],
      "metadata": {
        "id": "Pz-S7Up9AMyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "kqBDKSkuwlGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=load_model(\"/content/model1.h5\")"
      ],
      "metadata": {
        "id": "0Vf58hP5wEhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bleu score evaluation function\n",
        "def eval_model(model, descriptions, photos, word2ix, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, word2ix, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\t\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "KZ5QKVz2Ajtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing the test_set\n"
      ],
      "metadata": {
        "id": "ZUnDZNNxD5He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "#model1=\"/content/model1.h5\"\n",
        "#model = load_model(model1)\n",
        "# evaluate model\n",
        "eval_model(model, desc_train, test_features, word2ix, max_length)"
      ],
      "metadata": {
        "id": "9fzpPV2GA-f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPtPA0POc7Ma"
      },
      "source": [
        "##Greedy and Beam search\n",
        "def greedySearch(photo):\n",
        "  in_text = 'startseq'\n",
        "  for i in range(max_length):\n",
        "     sequence = [word2ix[w] for w in in_text.split() if w in word2ix]\n",
        "     sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "     yhat = model.predict([photo,sequence], verbose=0)\n",
        "     yhat = np.argmax(yhat)\n",
        "     word = ix2word[yhat]\n",
        "     in_text += ' ' + word\n",
        "     if word == 'endseq':\n",
        "       break\n",
        "  final = in_text.split()\n",
        "  final = final[1:-1]\n",
        "  final = ' '.join(final)\n",
        "  return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6eOeYaDdFv5"
      },
      "source": [
        "def beam_search_predictions(image, beam_index = 3):\n",
        "  start = [word2ix[\"startseq\"]]\n",
        "  start_word = [[start, 0.0]]\n",
        "  while len(start_word[0][0]) < max_length:\n",
        "    temp = []\n",
        "    for s in start_word:\n",
        "       par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "       preds = model.predict([image,par_caps], verbose=0)\n",
        "       word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "       # Getting the top <beam_index>(n) predictions and creating a \n",
        "       # new list so as to put them via the model again\n",
        "       for w in word_preds:\n",
        "         next_cap, prob = s[0][:], s[1]\n",
        "         next_cap.append(w)\n",
        "         prob += preds[0][w]\n",
        "         temp.append([next_cap, prob])\n",
        "    start_word = temp\n",
        "    # Sorting according to the probabilities\n",
        "    start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "    # Getting the top words\n",
        "    start_word = start_word[-beam_index:]\n",
        "  start_word = start_word[-1][0]\n",
        "  intermediate_caption = [ix2word[i] for i in start_word]\n",
        "  final_caption = []\n",
        "  for i in intermediate_caption:\n",
        "    if i != 'endseq':\n",
        "      final_caption.append(i)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  final_caption = ' '.join(final_caption[1:])\n",
        "  return final_caption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlIw2L_0dHTQ"
      },
      "source": [
        "#Evaluating using greedy and beam search\n",
        "pic = '2398605966_1d0c9e6a20.jpg'\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(imagepath+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Greedy Search:\",greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
        "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0klVlUey_meX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfDQvatSdHWI"
      },
      "source": [
        "#Taking few more examples\n",
        "pic = list(encoding_test.keys())[2]\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(imagepath+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Greedy:\",greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8zfy1MujgG4"
      },
      "source": [
        "#Taking few more examples\n",
        "pic = list(encoding_test.keys())[6]\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(imagepath+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Greedy:\",greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
        "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP0GD5iQkIGX"
      },
      "source": [
        "#Taking few more examples\n",
        "pic = list(encoding_test.keys())[11]\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(imagepath+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Greedy:\",greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
        "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXWYSS3adHZZ"
      },
      "source": [
        "#Taking few more examples\n",
        "pic = list(encoding_test.keys())[6]\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(imagepath+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Greedy:\",greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
        "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "c9Z1dkJ14j-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (0,5):\n",
        "  pic = list(encoding_test.keys())[i]\n",
        "  image = encoding_test[pic].reshape((1,2048))\n",
        "  x=plt.imread(imagepath+pic)\n",
        "  plt.imshow(x)\n",
        "  plt.show()\n",
        "  gs=greedySearch(image)\n",
        "  bs=beam_search_predictions(image, beam_index = 5)\n",
        "  #print(\"BLEUscore:\",corpus_bleu(gs, bs, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print(\"Greedy:\",gs)\n",
        "  print(\"Beam Search:\",bs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JSxe058F3s5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating using greedy and beam search\n",
        "pic = '2398605966_1d0c9e6a20.jpg'\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(imagepath+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Greedy Search:\",greedySearch(image))\n"
      ],
      "metadata": {
        "id": "6DQnE4U2_nvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}